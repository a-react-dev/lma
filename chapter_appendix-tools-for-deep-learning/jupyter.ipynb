{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "31d9a4c9",
      "metadata": {
        "origin_pos": 0,
        "id": "31d9a4c9"
      },
      "source": [
        "# Using Jupyter Notebooks\n",
        ":label:`sec_jupyter`\n",
        "\n",
        "\n",
        "This section describes how to edit and run the code\n",
        "in each section of this book\n",
        "using the Jupyter Notebook. Make sure you have\n",
        "installed Jupyter and downloaded the\n",
        "code as described in\n",
        ":ref:`chap_installation`.\n",
        "If you want to know more about Jupyter see the excellent tutorial in\n",
        "their [documentation](https://jupyter.readthedocs.io/en/latest/).\n",
        "\n",
        "\n",
        "## Editing and Running the Code Locally\n",
        "\n",
        "Suppose that the local path of the book's code is `xx/yy/d2l-en/`. Use the shell to change the directory to this path (`cd xx/yy/d2l-en`) and run the command `jupyter notebook`. If your browser does not do this automatically, open http://localhost:8888 and you will see the interface of Jupyter and all the folders containing the code of the book, as shown in :numref:`fig_jupyter00`.\n",
        "\n",
        "![The folders containing the code of this book.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter00.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter00`\n",
        "\n",
        "\n",
        "You can access the notebook files by clicking on the folder displayed on the webpage.\n",
        "They usually have the suffix \".ipynb\".\n",
        "For the sake of brevity, we create a temporary \"test.ipynb\" file.\n",
        "The content displayed after you click it is\n",
        "shown in :numref:`fig_jupyter01`.\n",
        "This notebook includes a markdown cell and a code cell. The content in the markdown cell includes \"This Is a Title\" and \"This is text.\".\n",
        "The code cell contains two lines of Python code.\n",
        "\n",
        "![Markdown and code cells in the \"text.ipynb\" file.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter01.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter01`\n",
        "\n",
        "\n",
        "Double click on the markdown cell to enter edit mode.\n",
        "Add a new text string \"Hello world.\" at the end of the cell, as shown in :numref:`fig_jupyter02`.\n",
        "\n",
        "![Edit the markdown cell.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter02.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter02`\n",
        "\n",
        "\n",
        "As demonstrated in :numref:`fig_jupyter03`,\n",
        "click \"Cell\" $\\rightarrow$ \"Run Cells\" in the menu bar to run the edited cell.\n",
        "\n",
        "![Run the cell.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter03.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter03`\n",
        "\n",
        "After running, the markdown cell is shown in :numref:`fig_jupyter04`.\n",
        "\n",
        "![The markdown cell after running.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter04.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter04`\n",
        "\n",
        "\n",
        "Next, click on the code cell. Multiply the elements by 2 after the last line of code, as shown in :numref:`fig_jupyter05`.\n",
        "\n",
        "![Edit the code cell.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter05.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter05`\n",
        "\n",
        "\n",
        "You can also run the cell with a shortcut (\"Ctrl + Enter\" by default) and obtain the output result from :numref:`fig_jupyter06`.\n",
        "\n",
        "![Run the code cell to obtain the output.](https://github.com/d2l-ai/d2l-en-colab/blob/master/img/jupyter06.png?raw=1)\n",
        ":width:`600px`\n",
        ":label:`fig_jupyter06`\n",
        "\n",
        "\n",
        "When a notebook contains more cells, we can click \"Kernel\" $\\rightarrow$ \"Restart & Run All\" in the menu bar to run all the cells in the entire notebook. By clicking \"Help\" $\\rightarrow$ \"Edit Keyboard Shortcuts\" in the menu bar, you can edit the shortcuts according to your preferences.\n",
        "\n",
        "## Advanced Options\n",
        "\n",
        "Beyond local editing two things are quite important: editing the notebooks in the markdown format and running Jupyter remotely.\n",
        "The latter matters when we want to run the code on a faster server.\n",
        "The former matters since Jupyter's native ipynb format stores a lot of auxiliary data that is\n",
        "irrelevant to the content,\n",
        "mostly related to how and where the code is run.\n",
        "This is confusing for Git, making\n",
        "reviewing contributions very difficult.\n",
        "Fortunately there is an alternative---native editing in the markdown format.\n",
        "\n",
        "### Markdown Files in Jupyter\n",
        "\n",
        "If you wish to contribute to the content of this book, you need to modify the\n",
        "source file (md file, not ipynb file) on GitHub.\n",
        "Using the notedown plugin we\n",
        "can modify notebooks in the md format directly in Jupyter.\n",
        "\n",
        "\n",
        "First, install the notedown plugin, run the Jupyter Notebook, and load the plugin:\n",
        "\n",
        "```\n",
        "pip install d2l-notedown  # You may need to uninstall the original notedown.\n",
        "jupyter notebook --NotebookApp.contents_manager_class='notedown.NotedownContentsManager'\n",
        "```\n",
        "\n",
        "You may also turn on the notedown plugin by default whenever you run the Jupyter Notebook.\n",
        "First, generate a Jupyter Notebook configuration file (if it has already been generated, you can skip this step).\n",
        "\n",
        "```\n",
        "jupyter notebook --generate-config\n",
        "```\n",
        "\n",
        "Then, add the following line to the end of the Jupyter Notebook configuration file (for Linux or macOS, usually in the path `~/.jupyter/jupyter_notebook_config.py`):\n",
        "\n",
        "```\n",
        "c.NotebookApp.contents_manager_class = 'notedown.NotedownContentsManager'\n",
        "```\n",
        "\n",
        "After that, you only need to run the `jupyter notebook` command to turn on the notedown plugin by default.\n",
        "\n",
        "### Running Jupyter Notebooks on a Remote Server\n",
        "\n",
        "Sometimes, you may want to run Jupyter notebooks on a remote server and access it through a browser on your local computer. If Linux or macOS is installed on your local machine (Windows can also support this function through third-party software such as PuTTY), you can use port forwarding:\n",
        "\n",
        "```\n",
        "ssh myserver -L 8888:localhost:8888\n",
        "```\n",
        "\n",
        "The above string `myserver` is the address of the remote server.\n",
        "Then we can use http://localhost:8888 to access the remote server `myserver` that runs Jupyter notebooks. We will detail on how to run Jupyter notebooks on AWS instances\n",
        "later in this appendix.\n",
        "\n",
        "### Timing\n",
        "\n",
        "We can use the `ExecuteTime` plugin to time the execution of each code cell in Jupyter notebooks.\n",
        "Use the following commands to install the plugin:\n",
        "\n",
        "```\n",
        "pip install jupyter_contrib_nbextensions\n",
        "jupyter contrib nbextension install --user\n",
        "jupyter nbextension enable execute_time/ExecuteTime\n",
        "```\n",
        "\n",
        "## Summary\n",
        "\n",
        "* Using the Jupyter Notebook tool, we can edit, run, and contribute to each section of the book.\n",
        "* We can run Jupyter notebooks on remote servers using port forwarding.\n",
        "\n",
        "\n",
        "## Exercises\n",
        "\n",
        "1. Edit and run the code in this book with the Jupyter Notebook on your local machine.\n",
        "1. Edit and run the code in this book with the Jupyter Notebook *remotely* via port forwarding.\n",
        "1. Compare the running time of the operations $\\mathbf{A}^\\top \\mathbf{B}$ and $\\mathbf{A} \\mathbf{B}$ for two square matrices in $\\mathbb{R}^{1024 \\times 1024}$. Which one is faster?\n",
        "\n",
        "\n",
        "[Discussions](https://discuss.d2l.ai/t/421)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentiment_RNN_Project.py\n",
        "# Full pipeline: Data loading -> cleaning -> EDA -> features -> LSTM -> evaluation\n",
        "# Run as a notebook (split cells) or script.\n",
        "\n",
        "import os\n",
        "import re\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_recall_fscore_support\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GRU, Dense, Dropout, BatchNormalization, Bidirectional\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "# ========== Configuration ==========\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "tf.random.set_seed(RANDOM_SEED)\n",
        "random.seed(RANDOM_SEED)\n",
        "\n",
        "file_path = '/content/Mini_Project/Week 19 - Graded Mini Project - Dataset - Twitter-training.csv'\n",
        "# If file has weird encoding, try: pd.read_csv(file_path, encoding='latin1', header=None)\n",
        "\n",
        "# ========== 1. Load Dataset ==========\n",
        "# Try to infer header; if not present, use header=None and then set names.\n",
        "try:\n",
        "    # Try reading with header first\n",
        "    df = pd.read_csv(file_path)\n",
        "    # If no error but columns are still numerical, it's likely no header was present\n",
        "    if df.columns.tolist()[:4] == [0,1,2,3]:\n",
        "         df = pd.read_csv(file_path, encoding='latin1', header=None) # Reload with no header\n",
        "except Exception:\n",
        "    # If initial read failed, try with encoding and no header\n",
        "    df = pd.read_csv(file_path, encoding='latin1', header=None)\n",
        "\n",
        "print(\"Raw shape:\", df.shape)\n",
        "display(df.head())\n",
        "\n",
        "# If the CSV has no proper column names, set them (example common format: id, topic, sentiment, text)\n",
        "# Based on the head output, columns seem to be: id, topic, sentiment, text\n",
        "# Check if columns are numerical (indicating no header was detected)\n",
        "if all(isinstance(col, int) for col in df.columns):\n",
        "    # Assume the order is id, topic, sentiment, text based on the original attempt and common format\n",
        "    new_column_names = ['id', 'topic', 'sentiment', 'text']\n",
        "    # Only assign if we have at least these many columns\n",
        "    if df.shape[1] >= len(new_column_names):\n",
        "        df.columns = new_column_names + df.columns[len(new_column_names):].tolist()\n",
        "\n",
        "# Now, select the relevant columns ('text' and 'sentiment')\n",
        "# Ensure the columns exist after potential renaming and filter out numeric sentiment values\n",
        "if 'text' in df.columns and 'sentiment' in df.columns:\n",
        "    # Filter out rows where 'sentiment' is not one of the expected string labels\n",
        "    expected_sentiments = ['Positive', 'Negative', 'Neutral', 'Irrelevant']\n",
        "    df = df[df['sentiment'].isin(expected_sentiments)].copy()\n",
        "    df = df[['text','sentiment']].dropna(subset=['text']).reset_index(drop=True)\n",
        "\n",
        "\n",
        "    # If text is split in multiple columns because of commas after renaming, merge remaining cols\n",
        "    # This part might be tricky if commas were within the original text, but let's try to handle it\n",
        "    # If the text column now contains lists or tuples due to the earlier merge attempt, flatten it\n",
        "    if df['text'].apply(type).nunique() > 1 or (df['text'].apply(type).iloc[0] != str and df['text'].apply(type).iloc[0] != np.object_):\n",
        "         # Assuming original issue was text split across numerical columns 3 onwards\n",
        "         if all(isinstance(col, int) for col in df.columns[:4]): # Check if initial load was header=None\n",
        "             # Reload specifically handling this case by joining columns from index 3 onwards\n",
        "             df = pd.read_csv(file_path, encoding='latin1', header=None)\n",
        "             if df.shape[1] >= 4:\n",
        "                  df.columns = ['id', 'topic', 'sentiment', 'text'] + [f'extra_{i}' for i in range(4, df.shape[1])]\n",
        "                  # Filter again after potential reload - IMPORTANT: Apply filtering of numeric sentiment here too\n",
        "                  expected_sentiments = ['Positive', 'Negative', 'Neutral', 'Irrelevant']\n",
        "                  df = df[df['sentiment'].isin(expected_sentiments)].copy()\n",
        "\n",
        "                  text_cols_to_merge = [col for col in df.columns if col.startswith('text') or col.startswith('extra_')]\n",
        "                  # Ensure text_cols_to_merge are in df.columns before selection\n",
        "                  text_cols_to_merge = [col for col in text_cols_to_merge if col in df.columns]\n",
        "                  if text_cols_to_merge: # Only merge if there are columns to merge\n",
        "                    df['text'] = df[text_cols_to_merge].astype(str).agg(' '.join, axis=1)\n",
        "                  else: # If no extra text columns, just use the designated 'text' column\n",
        "                    df['text'] = df['text'].astype(str)\n",
        "\n",
        "                  df = df[['sentiment', 'text']].dropna(subset=['text']).reset_index(drop=True)\n",
        "         else:\n",
        "             # Fallback if merging text failed and columns weren't purely numerical initially - IMPORTANT: Apply filtering of numeric sentiment here too\n",
        "              expected_sentiments = ['Positive', 'Negative', 'Neutral', 'Irrelevant']\n",
        "              df = df[df['sentiment'].isin(expected_sentiments)].copy()\n",
        "              df = df[['text','sentiment']].dropna(subset=['text']).reset_index(drop=True) # Revert to simple selection\n",
        "\n",
        "\n",
        "    df['text'] = df['text'].astype(str).str.strip()\n",
        "    print(\"After basic column normalization and sentiment filtering:\", df.shape)\n",
        "    display(df.head())\n",
        "else:\n",
        "    print(\"Error: 'text' or 'sentiment' columns not found after loading and renaming.\")\n",
        "    # Create an empty DataFrame or exit if essential columns are missing\n",
        "    df = pd.DataFrame(columns=['text', 'sentiment'])\n",
        "    print(\"Created empty DataFrame due to missing essential columns.\")\n",
        "\n",
        "\n",
        "# Remove sentiment classes with less than 2 samples for stratification\n",
        "if 'sentiment' in df.columns:\n",
        "    initial_shape = df.shape\n",
        "    sentiment_counts = df['sentiment'].value_counts()\n",
        "    classes_to_remove = sentiment_counts[sentiment_counts < 2].index.tolist()\n",
        "    if classes_to_remove:\n",
        "        print(f\"Removing sentiment classes with less than 2 samples: {classes_to_remove}\")\n",
        "        df = df[~df['sentiment'].isin(classes_to_remove)].copy()\n",
        "        print(f\"Shape after removing small classes: {df.shape}\")\n",
        "    else:\n",
        "        print(\"No sentiment classes with less than 2 samples found.\")\n",
        "else:\n",
        "    print(\"Skipping removal of small sentiment classes as 'sentiment' column is missing.\")\n",
        "\n",
        "\n",
        "# ========== 2. Data Cleaning & Preprocessing ==========\n",
        "# Download NLTK resources\n",
        "nltk.download('stopwords', quiet=True)\n",
        "nltk.download('punkt', quiet=True)\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "nltk.download('punkt_tab', quiet=True) # Add this line to download 'punkt_tab'\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_tweet(text):\n",
        "    \"\"\"\n",
        "    - Remove URLs, mentions, hashtags (or keep the word), emojis/special chars.\n",
        "    - Normalize: lowercase, remove extra whitespace.\n",
        "    - Tokenize & remove stopwords.\n",
        "    - Lemmatize.\n",
        "    \"\"\"\n",
        "    # Ensure text is a string\n",
        "    if not isinstance(text, str):\n",
        "        return \"\" # Return empty string for non-string inputs\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
        "    # Remove mentions (@user)\n",
        "    text = re.sub(r'@\\w+', '', text)\n",
        "    # Remove hashtags symbol only (keep the tag word)\n",
        "    text = re.sub(r'#', '', text)\n",
        "    # Remove HTML entities\n",
        "    text = re.sub(r'&\\w+;', '', text)\n",
        "    # Keep letters & common punctuation, remove others (including many emojis). If you want emojis preserved, adapt.\n",
        "    text = re.sub(r'[^A-Za-z0-9\\s\\.,!?\\'`]', ' ', text)\n",
        "    # Lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Check if text is empty after cleaning before tokenization\n",
        "    if not text.strip():\n",
        "        return \"\"\n",
        "\n",
        "    # Tokenize simple\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "\n",
        "    # Add a check for empty tokens after tokenization and before processing\n",
        "    tokens = [t for t in tokens if t and not t.isspace()]\n",
        "\n",
        "    # Remove stopwords and short tokens\n",
        "    tokens = [t for t in tokens if t not in stop_words and len(t) > 1]\n",
        "    # Lemmatize\n",
        "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Drop duplicates and missing\n",
        "if 'text' in df.columns and 'sentiment' in df.columns:\n",
        "    df = df.drop_duplicates().reset_index(drop=True)\n",
        "    print(\"After dropping duplicates:\", df.shape)\n",
        "    # Apply cleaning (this may take a while on large datasets)\n",
        "    df['clean_text'] = df['text'].apply(clean_tweet)\n",
        "    display(df[['text','clean_text']].head())\n",
        "else:\n",
        "    print(\"Skipping data cleaning and preprocessing due to missing columns.\")\n",
        "    # Create empty clean_text column if 'text' was not found\n",
        "    df['clean_text'] = \"\" # Ensure clean_text column exists\n",
        "\n",
        "\n",
        "# Optional: If you prefer stemming instead of lemmatization:\n",
        "# from nltk.stem.porter import PorterStemmer\n",
        "# stemmer = PorterStemmer()\n",
        "# tokens = [stemmer.stem(t) for t in tokens]\n",
        "\n",
        "# ========== 3. Feature Engineering ==========\n",
        "if 'clean_text' in df.columns and 'sentiment' in df.columns and not df.empty:\n",
        "    # A) TF-IDF features (for traditional ML or baseline)\n",
        "    tfidf = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
        "    X_tfidf = tfidf.fit_transform(df['clean_text'])\n",
        "\n",
        "    # B) Tokenizer + padded sequences for RNN\n",
        "    MAX_VOCAB = 15000\n",
        "    MAX_LEN = 80  # tune this\n",
        "\n",
        "    tokenizer = Tokenizer(num_words=MAX_VOCAB, oov_token='<OOV>')\n",
        "    tokenizer.fit_on_texts(df['clean_text'])\n",
        "    sequences = tokenizer.texts_to_sequences(df['clean_text'])\n",
        "    X_seq = pad_sequences(sequences, maxlen=MAX_LEN, padding='post', truncating='post')\n",
        "\n",
        "    # Label encoding\n",
        "    le = LabelEncoder()\n",
        "    # Inspect unique values before encoding\n",
        "    print(\"Unique values in 'sentiment' column:\", df['sentiment'].unique())\n",
        "    # Ensure sentiment column is string type before fitting LabelEncoder\n",
        "    y = le.fit_transform(df['sentiment'].astype(str))\n",
        "    print(\"Classes:\", le.classes_)\n",
        "\n",
        "    # ========== 5. Model Building (LSTM) ==========\n",
        "    if X_seq is not None and y is not None and le is not None and X_seq.shape[0] > 0:\n",
        "        # Split\n",
        "        # Ensure y has enough unique values for stratification if num_classes > 1\n",
        "        if len(np.unique(y)) > 1:\n",
        "            # Check unique values and counts in y right before splitting\n",
        "            unique_y, counts_y = np.unique(y, return_counts=True)\n",
        "            print(\"Unique values and counts in y before splitting:\", list(zip(unique_y, counts_y)))\n",
        "\n",
        "            X_train, X_test, y_train, y_test, seq_train, seq_test = train_test_split(\n",
        "                X_tfidf, y, X_seq, test_size=0.2, random_state=RANDOM_SEED, stratify=y) # Added stratify=y back as the issue might be resolved by filtering\n",
        "\n",
        "        else:\n",
        "             print(\"Only one class found. Skipping stratification.\")\n",
        "             X_train, X_test, y_train, y_test, seq_train, seq_test = train_test_split(\n",
        "                X_tfidf, y, X_seq, test_size=0.2, random_state=RANDOM_SEED)\n",
        "\n",
        "\n",
        "        # For RNN we use seq_train/seq_test\n",
        "        # Define a simple Embedding + BiLSTM model\n",
        "        vocab_size = min(MAX_VOCAB, len(tokenizer.word_index) + 1)\n",
        "        embedding_dim = 100\n",
        "\n",
        "        def build_lstm_model(vocab_size=vocab_size, embedding_dim=embedding_dim, input_length=MAX_LEN, lstm_units=128, dropout_rate=0.5, num_classes=None):\n",
        "            if num_classes is None:\n",
        "                num_classes = len(np.unique(y))\n",
        "            model = Sequential()\n",
        "            model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length))\n",
        "            model.add(Bidirectional(LSTM(lstm_units, return_sequences=False)))\n",
        "            model.add(BatchNormalization())\n",
        "            model.add(Dropout(dropout_rate))\n",
        "            if num_classes == 2:\n",
        "                model.add(Dense(1, activation='sigmoid'))\n",
        "                loss = 'binary_crossentropy'\n",
        "            else:\n",
        "                model.add(Dense(num_classes, activation='softmax'))\n",
        "                loss = 'sparse_categorical_crossentropy'\n",
        "            model.compile(optimizer='adam', loss=loss, metrics=['accuracy'])\n",
        "            return model\n",
        "\n",
        "        num_classes = len(np.unique(y))\n",
        "        model = build_lstm_model(num_classes=num_classes)\n",
        "        model.summary()\n",
        "\n",
        "        # Callbacks\n",
        "        es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "        # Optionally save best model:\n",
        "        checkpoint_path = 'best_lstm.h5'\n",
        "        mc = ModelCheckpoint(checkpoint_path, monitor='val_loss', save_best_only=True)\n",
        "\n",
        "        # Train\n",
        "        BATCH_SIZE = 64\n",
        "        EPOCHS = 10\n",
        "\n",
        "        history = model.fit(\n",
        "            seq_train, y_train,\n",
        "            validation_split=0.1,\n",
        "            epochs=EPOCHS,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            callbacks=[es, mc],\n",
        "            verbose=1\n",
        "        )\n",
        "    else:\n",
        "        print(\"Skipping model building due to missing data, labels, or insufficient data for splitting.\")\n",
        "        model = None\n",
        "        history = None\n",
        "\n",
        "else:\n",
        "    print(\"Skipping feature engineering and model building due to missing columns or empty DataFrame.\")\n",
        "    # Initialize empty variables to avoid errors later\n",
        "    X_tfidf = None\n",
        "    X_seq = None\n",
        "    y = None\n",
        "    le = None\n",
        "    model = None\n",
        "    history = None\n",
        "\n",
        "\n",
        "# ========== 4. EDA ==========\n",
        "if df is not None and 'sentiment' in df.columns and 'clean_text' in df.columns and not df.empty:\n",
        "    # Basic stats\n",
        "    print(\"Total tweets:\", df.shape[0])\n",
        "    print(df['sentiment'].value_counts())\n",
        "\n",
        "    # Sentiment distribution plots\n",
        "    plt.figure(figsize=(6,4))\n",
        "    sns.countplot(x='sentiment', data=df, order=df['sentiment'].value_counts().index)\n",
        "    plt.title('Sentiment Distribution')\n",
        "    plt.xlabel('Sentiment')\n",
        "    plt.ylabel('Count')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Pie chart\n",
        "    plt.figure(figsize=(6,6))\n",
        "    df['sentiment'].value_counts().plot.pie(autopct='%1.1f%%', startangle=90, ylabel='')\n",
        "    plt.title('Sentiment Proportions')\n",
        "    plt.show()\n",
        "\n",
        "    # Word clouds for positive/negative (if those classes exist)\n",
        "    for sentiment in df['sentiment'].unique():\n",
        "        text_blob = ' '.join(df.loc[df['sentiment']==sentiment, 'clean_text'].values)\n",
        "        if len(text_blob.strip()) == 0:\n",
        "            print(f\"No clean text found for sentiment: {sentiment}\")\n",
        "            continue\n",
        "        wc = WordCloud(width=800, height=400, background_color='white').generate(text_blob)\n",
        "        plt.figure(figsize=(10,4))\n",
        "        plt.imshow(wc, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title(f'WordCloud for {sentiment}')\n",
        "        plt.show()\n",
        "\n",
        "    # Top keywords per sentiment using TF-IDF or simple frequency\n",
        "    from collections import Counter\n",
        "    def top_n_words(texts, n=20):\n",
        "        cnt = Counter()\n",
        "        for t in texts:\n",
        "            cnt.update(t.split())\n",
        "        return cnt.most_common(n)\n",
        "\n",
        "    for s in df['sentiment'].unique():\n",
        "        print(\"\\nTop words for\", s)\n",
        "        texts_for_sentiment = df.loc[df['sentiment']==s, 'clean_text']\n",
        "        if not texts_for_sentiment.empty:\n",
        "             print(top_n_words(texts_for_sentiment, 15))\n",
        "        else:\n",
        "            print(f\"No clean text found for sentiment: {s}\")\n",
        "\n",
        "\n",
        "    # Relationship between tweet length and sentiment\n",
        "    df['clean_len'] = df['clean_text'].apply(lambda x: len(x.split()))\n",
        "    plt.figure(figsize=(8,5))\n",
        "    sns.boxplot(x='sentiment', y='clean_len', data=df)\n",
        "    plt.title('Tweet Length (tokens) by Sentiment')\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Skipping EDA due to missing columns, empty DataFrame, or data.\")\n",
        "\n",
        "\n",
        "# ========== 6. Evaluation ==========\n",
        "# Check if model, test data, and labels are available and valid before evaluating\n",
        "if model is not None and X_test is not None and y_test is not None and le is not None and seq_test is not None and seq_test.shape[0] > 0:\n",
        "    # Predict\n",
        "    y_pred_prob = model.predict(seq_test)\n",
        "    if num_classes == 2:\n",
        "        y_pred = (y_pred_prob.flatten() > 0.5).astype(int)\n",
        "    else:\n",
        "        y_pred = np.argmax(y_pred_prob, axis=1)\n",
        "\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    # Ensure target_names match the classes present in y_test if subset was used\n",
        "    try:\n",
        "        print(\"Classification Report:\")\n",
        "        print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
        "    except ValueError as e:\n",
        "        print(f\"Could not generate classification report with original classes. Error: {e}\")\n",
        "        # Fallback report without specific target names if issue with label mapping\n",
        "        print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "    plt.figure(figsize=(6,5))\n",
        "    # Handle potential mismatch between unique values in y_test and le.classes_ for heatmap labels\n",
        "    unique_y_test = np.unique(y_test)\n",
        "    cm_xticklabels = [le.classes_[i] for i in unique_y_test] if len(unique_y_test) <= len(le.classes_) else unique_y_test\n",
        "    cm_yticklabels = [le.classes_[i] for i in unique_y_test] if len(unique_y_test) <= len(le.classes_) else unique_y_test\n",
        "\n",
        "    sns.heatmap(cm, annot=True, fmt='d', xticklabels=cm_xticklabels, yticklabels=cm_yticklabels, cmap='Blues')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('True')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot learning curves\n",
        "    if history is not None and history.history:\n",
        "        plt.figure(figsize=(12,4))\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.plot(history.history.get('loss'), label='train_loss')\n",
        "        plt.plot(history.history.get('val_loss'), label='val_loss')\n",
        "        plt.legend(); plt.title('Loss')\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.plot(history.history.get('accuracy'), label='train_acc')\n",
        "        plt.plot(history.history.get('val_accuracy'), label='val_acc')\n",
        "        plt.legend(); plt.title('Accuracy')\n",
        "        plt.show()\n",
        "    elif history is None:\n",
        "        print(\"Skipping plotting learning curves as history is not available.\")\n",
        "    else:\n",
        "        print(\"Skipping plotting learning curves as history object is empty.\")\n",
        "\n",
        "else:\n",
        "    print(\"Skipping evaluation due to missing model, test data, labels, or insufficient test data points.\")\n",
        "\n",
        "\n",
        "# ========== 7. Model Improvement & Hyperparameter Tuning (outline) ==========\n",
        "# - Use GridSearch on number of LSTM units, dropout, embedding dim (but heavy to run).\n",
        "# - Use pretrained embeddings (GloVe): create embedding_matrix and use Embedding(..., weights=[embedding_matrix], trainable=False/True)\n",
        "# - Try Transformers (e.g., HuggingFace BERT) for transfer learning (much better results, requires additional libraries).\n",
        "\n",
        "# Example: quick grid-search skeleton (won't run here as-is due to Keras model compatibility with sklearn)\n",
        "# You can implement KerasClassifier wrapper or manual loops over params:\n",
        "# for units in [64,128]:\n",
        "#     for drop in [0.2,0.5]:\n",
        "#         model = build_lstm_model(lstm_units=units, dropout_rate=drop)\n",
        "#         history = model.fit(...)\n",
        "\n",
        "# ========== 8. Save artifacts ==========\n",
        "if tokenizer is not None and tfidf is not None and model is not None and hasattr(tokenizer, 'word_index'):\n",
        "    import pickle\n",
        "    try:\n",
        "        with open('tokenizer.pkl','wb') as f:\n",
        "            pickle.dump(tokenizer, f)\n",
        "        print(\"Tokenizer saved.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not save tokenizer: {e}\")\n",
        "\n",
        "    try:\n",
        "        with open('tfidf.pkl','wb') as f:\n",
        "            pickle.dump(tfidf, f)\n",
        "        print(\"TF-IDF vectorizer saved.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not save TF-IDF vectorizer: {e}\")\n",
        "\n",
        "    try:\n",
        "        model.save('final_lstm_model.h5')\n",
        "        print(\"Keras model saved.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Could not save Keras model: {e}\")\n",
        "else:\n",
        "    print(\"Skipping saving artifacts due to missing components or invalid tokenizer.\")\n",
        "\n",
        "\n",
        "# ========== 9. Quick Inference function for live demo ==========\n",
        "if tokenizer is not None and model is not None and le is not None and hasattr(tokenizer, 'word_index'):\n",
        "    def predict_sentiment(text):\n",
        "        # Ensure num_classes is correctly inferred or passed\n",
        "        num_classes_in_model = model.output_shape[-1] if model.output_shape is not None else (len(le.classes_) if le is not None else 2)\n",
        "\n",
        "        clean = clean_tweet(text)\n",
        "        seq = tokenizer.texts_to_sequences([clean])\n",
        "        pad = pad_sequences(seq, maxlen=MAX_LEN, padding='post')\n",
        "        prob = model.predict(pad)\n",
        "        if num_classes_in_model == 1 or (le is not None and len(le.classes_) == 2): # Handle binary explicitly\n",
        "            # Assuming sigmoid output for binary\n",
        "            label_idx = int(prob.flatten()[0] > 0.5)\n",
        "            score = float(prob.flatten()[0])\n",
        "            label = le.inverse_transform([label_idx])[0] if le is not None else (\"Positive\" if label_idx == 1 else \"Negative\")\n",
        "        elif num_classes_in_model > 1:\n",
        "            # Assuming softmax output for multi-class\n",
        "            idx = np.argmax(prob, axis=1)[0]\n",
        "            label = le.inverse_transform([idx])[0] if le is not None else f\"Class {idx}\"\n",
        "            score = float(prob[0, idx])\n",
        "        else:\n",
        "             return \"Could not predict\", 0.0 # Fallback\n",
        "\n",
        "        return label, score\n",
        "\n",
        "    # Demo\n",
        "    samples = [\n",
        "        \"I love this game, it's so fun!\",\n",
        "        \"This update ruined everything. Worst patch ever.\",\n",
        "        \"Meh, it's okay I guess.\"\n",
        "    ]\n",
        "    print(\"\\n--- Inference Demo ---\")\n",
        "    for s in samples:\n",
        "        # Check if all components for prediction are available\n",
        "        if tokenizer and model and le and hasattr(tokenizer, 'word_index'):\n",
        "            print(s, \"->\", predict_sentiment(s))\n",
        "        else:\n",
        "             print(f\"Skipping demo for '{s}' due to missing inference components.\")\n",
        "else:\n",
        "    print(\"Skipping inference demo due to missing components or invalid tokenizer.\")\n",
        "\n",
        "\n",
        "# ========== 10. Report & Slides ==========\n",
        "# For the report: export figures generated above and create a short slide deck (e.g., using python-pptx) summarizing:\n",
        "# - Dataset overview\n",
        "# - EDA highlights (charts + wordclouds)\n",
        "# - Model architecture screenshot/summary\n",
        "# - Evaluation metrics\n",
        "# - Demo predictions\n",
        "# (Implementation of ppt generation is optional; can be done with python-pptx.)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "KkOy7h8Mbrb9",
        "outputId": "587634ff-df28-49a8-8e5b-e960816aff0f"
      },
      "id": "KkOy7h8Mbrb9",
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw shape: (74681, 4)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   2401  Borderlands  Positive  \\\n",
              "0  2401  Borderlands  Positive   \n",
              "1  2401  Borderlands  Positive   \n",
              "2  2401  Borderlands  Positive   \n",
              "3  2401  Borderlands  Positive   \n",
              "4  2401  Borderlands  Positive   \n",
              "\n",
              "  im getting on borderlands and i will murder you all ,  \n",
              "0  I am coming to the borders and I will kill you...     \n",
              "1  im getting on borderlands and i will kill you ...     \n",
              "2  im coming on borderlands and i will murder you...     \n",
              "3  im getting on borderlands 2 and i will murder ...     \n",
              "4  im getting into borderlands and i can murder y...     "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b2687f75-928c-418f-9e65-c6370811b8a5\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>2401</th>\n",
              "      <th>Borderlands</th>\n",
              "      <th>Positive</th>\n",
              "      <th>im getting on borderlands and i will murder you all ,</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2401</td>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Positive</td>\n",
              "      <td>I am coming to the borders and I will kill you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2401</td>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Positive</td>\n",
              "      <td>im getting on borderlands and i will kill you ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2401</td>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Positive</td>\n",
              "      <td>im coming on borderlands and i will murder you...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2401</td>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Positive</td>\n",
              "      <td>im getting on borderlands 2 and i will murder ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2401</td>\n",
              "      <td>Borderlands</td>\n",
              "      <td>Positive</td>\n",
              "      <td>im getting into borderlands and i can murder y...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b2687f75-928c-418f-9e65-c6370811b8a5')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-b2687f75-928c-418f-9e65-c6370811b8a5 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-b2687f75-928c-418f-9e65-c6370811b8a5');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-cde8ae38-c0b3-46a0-9e52-0f6a2e8143f6\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cde8ae38-c0b3-46a0-9e52-0f6a2e8143f6')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-cde8ae38-c0b3-46a0-9e52-0f6a2e8143f6 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"# (Implementation of ppt generation is optional; can be done with python-pptx\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"2401\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 2401,\n        \"max\": 2401,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          2401\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Borderlands\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Borderlands\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Positive\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Positive\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"im getting on borderlands and i will murder you all ,\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"im getting on borderlands and i will kill you all,\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 'text' or 'sentiment' columns not found after loading and renaming.\n",
            "Created empty DataFrame due to missing essential columns.\n",
            "No sentiment classes with less than 2 samples found.\n",
            "After dropping duplicates: (0, 2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Empty DataFrame\n",
              "Columns: [text, clean_text]\n",
              "Index: []"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c03d8664-6f45-4ed1-aef9-e39581d6a2ad\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c03d8664-6f45-4ed1-aef9-e39581d6a2ad')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c03d8664-6f45-4ed1-aef9-e39581d6a2ad button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c03d8664-6f45-4ed1-aef9-e39581d6a2ad');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping feature engineering due to missing columns or empty DataFrame.\n",
            "Skipping EDA due to missing columns, empty DataFrame, or data.\n",
            "Skipping model building due to missing data, labels, or insufficient data for splitting.\n",
            "Skipping evaluation due to missing model, test data, labels, or insufficient test data points.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'tokenizer' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3512464958.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;31m# ========== 8. Save artifacts ==========\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtfidf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word_index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "required_libs": [],
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}